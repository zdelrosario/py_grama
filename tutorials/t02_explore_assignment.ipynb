{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 02: Exploratory Model Analysis\n",
    "\n",
    "*Authors: Zach del Rosario*\n",
    "\n",
    "---\n",
    "\n",
    "This is a tutorial on using grama to do *exploratory model analysis*; to evaluate the model to generate data, then use that data to understand the model.\n",
    "\n",
    "**Learning Goals**: By completing this notebook, you will learn:\n",
    "1. How to use the verbs `gr.eval_monte_carlo` and `gr.eval_sinews`\n",
    "1. How to use `gr.plot_auto`\n",
    "1. Common grama arguments and defaults\n",
    "\n",
    "**Prerequisites**:\n",
    "- Familiarity with the Python programming language\n",
    "- [Tutorial 01: Introduction]()\n",
    "\n",
    "**Table of Contents**:\n",
    "1. [Initialize](#s1)\n",
    "2. [Monte Carlo Simulation](#s2)\n",
    "3. [Sweeps](#s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize <a id=\"s1\"></a>\n",
    "\n",
    "In order to perform model analysis, we first need to construct the model. For this exercise we'll use a pre-built model: the cantilever beam model.\n",
    "\n",
    "#### __Q1: Initialize grama__\n",
    "Import grama and the cantilever beam model.\n",
    "\n",
    "*Hint*: We initialized grama in the previous notebook; feel free to copy and paste from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Set up grama\n",
    "# TODO: Import grama, make the cantilever_beam model\n",
    "###\n",
    "\n",
    "# TODO: Import grama\n",
    "# TODO: Assign the cantilever_beam model to `md`\n",
    "\n",
    "\n",
    "# -- NO NEED TO MODIFY BELOW ----\n",
    "md.printpretty()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Simulation <a id=\"s2\"></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q2: Monte Carlo__\n",
    "Perform a Monte Carlo simulation on model `md` with `gr.eval_monte_carlo`. Draw `100` samples, and use the nominal settings for the deterministic variables. Determine which arguments are required, and which are optional.\n",
    "\n",
    "*Hint*: In Jupyter, click-selecting a function and pressing `Shift + Tab` will bring up the documentation. Use this to investigate the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Perform a monte carlo simulation (MCS)\n",
    "# TODO: Use gr.eval_monte_carlo, determine which arguments you need to set\n",
    "###\n",
    "\n",
    "# TODO: Perform MCS, assign results to `df_mc`\n",
    "\n",
    "\n",
    "# -- NO NEED TO MODIFY BELOW ----\n",
    "df_mc.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q3: Random Seeds__\n",
    "Run the code cell above a few times, and note how the results change. Then add the `seed` keyword argument with your favorite integer, and try again.\n",
    "\n",
    "Random seeds are useful when debugging Monte Carlo results, as they ensure the same \"random\" results on repeated execution. As a rough rule of thumb you should systematically use multiple seeds when testing algorithms, but fix one seed when studying a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q4: Skip evaluation__\n",
    "Modify your code above, and use the `skip` keyword to skip evaluating the functions. Take the results of `gr.eval_monte_carlo` and pass them to `gr.plot_auto`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Skip evaluation\n",
    "# TODO: Use gr.eval_monte_carlo with the skip keyword\n",
    "###\n",
    "\n",
    "# TODO: Perform MCS with skipped evaluation, assign results to `df_skip`\n",
    "\n",
    "\n",
    "gr.plot_auto(df_skip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the autoplotter with skipped evaluation provides a visualization of the *design of experiment* (DOE), or sampling plan. Note that `gr.eval_monte_carlo` also provides an estimate of the runtime of the DOE paired with the chosen model---this is only possible when the model as runtime estimates available. When studying more expensive models, running a `skip` check first to inspect the design is often a good idea: This practice can help you catch errors before using a lot of compute resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q5: Autoplot evaluation__\n",
    "Modify your code above to evaluate the model functions. Take the results of `gr.eval_monte_carlo` and pass them to `gr.plot_auto`. Use the same seed as you used above when setting `skip=True`. Interpret the resulting histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Autoplot MCS\n",
    "# TODO: Use gr.eval_monte_carlo with gr.plot_auto\n",
    "###\n",
    "\n",
    "# TODO: Perform MCS and visualize with gr.plot_auto\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the MCS output histograms, you should be able to see that `c_area` is unaffected by the random variables, while `g_stress` and `g_disp` have a small faction of cases which lead to negative values. Since we used the same `seed` for the skipped and evaluated cases, we can guarantee the input design above matches the output results here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweeps <a id=\"s3\"></a>\n",
    "    \n",
    "---    \n",
    "\n",
    "Monte Carlo Simulation is very useful for estimating distributions and probabilities. However, sometimes we want a more qualitative understanding of the random variables' impact on model outputs. In this last section we will use *sweeps* to gain some qualitative understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q6: Sinew Design__\n",
    "Use the verb `gr.eval_sinews` to construct a sinew DOE. Visualize the design without evaluating. Describe the DOE in words.\n",
    "\n",
    "*Hint*: Use the same patterns we used for `gr.eval_monte_carlo` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Sinew design\n",
    "# TODO: Use gr.eval_sinews to generate a design\n",
    "###\n",
    "\n",
    "# TODO: Generate a sinew design but do not evaluate the model functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Q7: Sinew Study__\n",
    "Use the verb `gr.eval_sinews` to evaluate the model. Visualize and interpret the results.\n",
    "\n",
    "*Hint*: Use the same patterns we used for `gr.eval_monte_carlo` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Sinew evaluation\n",
    "# TODO: Use gr.eval_sinews to evaluate the model\n",
    "###\n",
    "\n",
    "# TODO: Generate, evaluate, and visualize a sinew design\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
