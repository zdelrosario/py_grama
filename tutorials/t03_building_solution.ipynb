{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 03: Model Building\n",
    "\n",
    "*Authors: Zach del Rosario*\n",
    "\n",
    "---\n",
    "\n",
    "This is a tutorial on using grama to build a *fully-defined* model. A grama model considers a function and its sources of uncertainty. To carry out [exploratory model analysis](https://github.com/zdelrosario/py_grama/blob/master/tutorials/t02_explore_assignment.ipynb), we need all this information.\n",
    "\n",
    "**Learning Goals**: By completing this notebook, you will learn:\n",
    "1. How to fully-define a grama model\n",
    "1. The importance of *exploratory data analysis* for model building\n",
    "1. Utility functions in `py_grama` for fitting a distribution\n",
    "1. Best-practices for building a trustworthy model in a communicable way\n",
    "\n",
    "**Prerequisites**:\n",
    "- Familiarity with the Python programming language\n",
    "- [Tutorial 01: Introduction](https://github.com/zdelrosario/py_grama/blob/master/tutorials/t01_introduction_assignment.ipynb)\n",
    "\n",
    "**Table of Contents**:\n",
    "1. [Anatomy of a model](#s1)\n",
    "1. [Defining a function](#s2)\n",
    "1. [Defining a distribution](#s3)\n",
    "   1. [Exploratory data analysis](#s3.1)\n",
    "   1. [Fitting marginals](#s3.2)\n",
    "   1. [Fitting a copula](#s3.3)\n",
    "1. [Evaluating the model](#s4) (Illustration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a model <a id=\"s1\"></a>\n",
    "\n",
    "To remind ourselves of the anatomy of a grama model, we'll first inspect a pre-defined model.\n",
    "\n",
    "#### __Q1: Initialize grama__\n",
    "Import grama and the cantilever beam model.\n",
    "\n",
    "*Hint*: We initialized grama in Tutorial 1, but see if you can remember the appropriate calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Set up grama\n",
    "# TODO: Import grama, make the cantilever_beam model\n",
    "###\n",
    "\n",
    "\n",
    "import grama as gr\n",
    "from grama.models import make_cantilever_beam\n",
    "\n",
    "# -- NO NEED TO MODIFY BELOW ----\n",
    "md = make_cantilever_beam()\n",
    "md.printpretty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `printpretty` output above illustrates the necessary elements of a complete grama model:\n",
    "\n",
    "- The function `inputs`, which include `variables`, which are further are categorized into deterministic (`var_det`) and random (`var_rand`) categories\n",
    "- Distribution information for the random variables, including marginal distribution shapes (`norm`), their parameters (`loc, scale`), and a dependence structure (`copula`).\n",
    "- The functions, which map `variables` to `outputs`. \n",
    "\n",
    "*General principle*: Since the functions define *which inputs are necessary*, and the distributions define *facts about inputs*, it is generally advisable to start by defining the model functions, then define the distribution. We will follow this suggested order below.\n",
    "\n",
    "In real grama code, fully-defining a model will look like the following:\n",
    "\n",
    "```python\n",
    "md = (\n",
    "    gr.Model(\"Model name\")\n",
    "    >> gr.cp_function(\n",
    "        fun=callable_function,\n",
    "        var=[\"x1\", \"x2\", \"x3\"],\n",
    "        out=[\"output1\", \"output2\"],\n",
    "        name=\"Human-readable function name\"\n",
    "    )\n",
    "    >> gr.cp_bounds(\n",
    "        x1=(-1, +1) # Bounds for deterministic variable\n",
    "    )\n",
    "    >> gr.cp_marginals(\n",
    "        x2=gr.continuous_fit(df_data.x2, \"norm\") # Fit a normal dist\n",
    "        x3=gr.continuous_fit(df_data.x3, \"beta\") # Fit a beta dist\n",
    "    )\n",
    "    >> gr.cp_copula_independence() # Specify dependence structure\n",
    ")\n",
    "```\n",
    "\n",
    "After every composition call, we are left with a *valid* grama model. However, the model will only have certain features available at the various stages of (in)completeness. We will walk through building up this model in a *stepwise* fashion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Goal: Buckling Plate Model\n",
    "\n",
    "---\n",
    "\n",
    "To accomplish the learning goals of this tutorial, we will build a model for the buckling failure of a metallic plate. \n",
    "\n",
    "#### Model purpose\n",
    "\n",
    "**The most important part of a model is its intended purpose**---this informs all decisions about the model, and serves as our center for assessing model efficacy.\n",
    "\n",
    "In this exercise, we state the following purpose:\n",
    "\n",
    "> The purpose of this model is to support the evaluation of an Aluminum plate stock supplier. Our manufacturing process includes forming of the stock with good tolerance control, and the plates are known to be buckling-critical (with free edges) in their intended use-case. The geometry of these plates is fixed and known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Physical model\n",
    "Since we are ultimately interested in the buckling of plates, we need a physical model for this process. Reference [1] provides a model for the critical buckling stress of a plate with free edges under compressive load. This information is organized into a so-called \"limit state function\" $g_{\\text{buckling}}$ below.\n",
    "\n",
    "$$g_{\\text{buckling}} = \\frac{\\pi E}{12 (1 - \\mu^2)} \\left(\\frac{t}{h}\\right)^2 - \\frac{L}{wt}$$\n",
    "\n",
    "The limit state summarizes information about a structure in terms of *failure*; it takes values corresponding to:\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "| $g > 0$ | Safe |\n",
    "| $g \\leq 0$ | Failure |\n",
    "\n",
    "The inputs for this function are disambiguated in the table below:\n",
    "\n",
    "| Variable | Symbol | Units |\n",
    "|---|---|---\n",
    "| Thickness | $t$ | in |\n",
    "| Height | $h$ | in |\n",
    "| Width | $w$ | in |\n",
    "| Elastic modulus | $E$ | kips / sq-in |\n",
    "| Poisson ratio | $\\mu$ | - |\n",
    "| Applied load | $L$ | kips |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function <a id=\"s2\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "Defining a model is accomplished through *compositions*; these grama verbs take a model as input and return a new model as an output. Composition verbs begin with the `comp_ (cp_)` prefix, and are our go-to tool for defining a model in *stages*. \n",
    "\n",
    "#### __Q2: Implement and add the function__\n",
    "\n",
    "Use `gr.Model()` and `gr.comp_function()` to start the model-building process. Implement the equation above and assign it to the model `md_plate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Begin the model\n",
    "# TODO: Use gr.Model() to start a model md_plate, use\n",
    "#       gr.comp_function() to add a function. Make sure\n",
    "#       to provide the correct variable names to\n",
    "#       gr.comp_function() in the correct order, as\n",
    "#       well as the correct output name \"g_buckle\".\n",
    "# NOTE: If your implementation is correct, the\n",
    "#       test below will pass.\n",
    "###\n",
    "\n",
    "\n",
    "def function_buckle_state(x):\n",
    "    t, h, w, E, mu, L = x\n",
    "    return pi * E / 12 / (1 - mu**2) * (t / h) ** 2 - L / t / w\n",
    "\n",
    "\n",
    "md_plate = gr.Model(\"Plate buckling\") >> gr.cp_function(\n",
    "    fun=function_buckle_state, var=var_list, out=out, name=\"limit state\"\n",
    ")\n",
    "# -- NO NEED TO MODIFY BELOW ----\n",
    "# Run test for correctness\n",
    "from grama.models import make_plate_buckle\n",
    "import pandas as pd\n",
    "\n",
    "md_ref = make_plate_buckle()\n",
    "df_ref = gr.eval_nominal(md_ref, df_det=\"nom\")\n",
    "df_test = gr.eval_df(md_plate, df_ref[md_plate.var])\n",
    "\n",
    "if gr.df_equal(df_ref, df_test):\n",
    "    print(\"Test passed!\")\n",
    "else:\n",
    "    print(\"Test failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the test above calls `gr.eval_df` on your model. The model is not yet fully-defined, but we can still evaluate its functions if we exactly specify all its input values. We do this above by loading a reference model (`md_ref`), evaluating the model to get its nominal input values, and checking that the outputs match between the two implementations. We can think of this as a very simple *software verification* check---a check that we have implemented the function correctly. Here we compare against a reference implementation; in practice you may want to check against experimental or simulation data, or perform sweeps (see [T02: EMA](https://github.com/zdelrosario/py_grama/blob/master/tutorials/t02_explore_assignment.ipynb)) to check that the model behavior is plausible.\n",
    "\n",
    "A *useful* thing we can add to our model are bounds for the inputs. These optional values *do not* actually limit what values can be used to evaluate; instead, they provide the analyst some context for what values the model is *expected* to be used. We will practice adding bounds to our model below.\n",
    "\n",
    "#### __Q3: Add bounds__\n",
    "\n",
    "Use `gr.comp_bounds()` to add bounds for the inputs. Evaluate your model at its nominal values, and note what input values are returned for the variables you bound, and those you do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Add bounds\n",
    "# TODO: Add bounds to your model md_plate with\n",
    "#       gr.comp_bounds(); use gr.eval_nominal() to\n",
    "#       evaluate.\n",
    "###\n",
    "\n",
    "\n",
    "md_plate = md_plate >> gr.cp_bounds(t=(0.125, 0.25), w=(6, 18), h=(6, 18))\n",
    "gr.eval_nominal(md_plate, df_det=\"nom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing bounds implicitly defines nominal values for the model, which are used by the shortcut utility `gr.eval_nominal()`. For a given model, the nominal values for the deterministic inputs will be the same for any grama verb that takes the `df_det` keyword argument.\n",
    "\n",
    "At this point, `md_plate` can be used to evaluate its function, but cannot be used for studying uncertainty associated with the model. To illustrate, try executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- UNCOMMENT AND EXECUTE THIS CODE ----\n",
    "# gr.eval_monte_carlo(md_plate, df_det=\"nom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that attempting a Monte Carlo analysis of the model results in an error message; the *distribution* for this model is not defnined, so a Monte Carlo has no meaning! We will fix this below.\n",
    "\n",
    "Now that we have defined all the functions for our model, the full set of variables is fixed, and we can move on to defining a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a distribution <a id=\"s3\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "In this section we define a distribution for the model variables. There are many ways to do this; in this example we will use experimental data to fit a joint distribution.\n",
    "\n",
    "As a first step, we must first decide which variables will be *deterministic*, and which will be *random*. Note that by selecting a variable to be deterministic, we are asserting that its value can be _exactly_ known. If this is not an appropriate assumption, then a random variable model is more appropriate. \n",
    "\n",
    "Note that we can model a variable as random, perform a model analysis, and determine that the variable is unimportant for our intended analysis purposes---in this case we could then modify the model to make the variable deterministic. The important takeaway is that we need to make principled, defensible modeling decisions grounded in our intended application.\n",
    "\n",
    "To illustrate, let's return to the table of variables for our plate model.\n",
    "\n",
    "| Input | Symbol | Units |\n",
    "|---|---|---\n",
    "| Thickness | $t$ | in |\n",
    "| Height | $h$ | in |\n",
    "| Width | $w$ | in |\n",
    "| Elastic modulus | $E$ | kips / sq-in |\n",
    "| Poisson ratio | $\\mu$ | - |\n",
    "| Applied load | $L$ | kips |\n",
    "\n",
    "Recall our intended model purpose:\n",
    "\n",
    "> The purpose of this model is to support the evaluation of an Aluminum plate stock supplier. Our manufacturing process includes forming of the stock with good tolerance control, and the plates are known to be buckling-critical (with free edges) in their intended use-case. The geometry of these plates is fixed and known.\n",
    "\n",
    "Based on this purpose, we argue the following:\n",
    "\n",
    "- Thickness, height, and width (`t,h,w`) will be deterministic, as we have good tolerance control and fixed, known geometry.\n",
    "- The load (`L`) is determined by our use-case and not the supplier, therefore we will also treat it as deterministic.\n",
    "- The Elastic modulus and Poisson ratio (`E, mu`) vary during the manufacturing of the raw stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis <a id=\"s3.1\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "Ideally, random variable models (joint distributions) should be *defined in a principled fashion*. Valid approaches include using experimental data, consulting literature, and consulting an expert. Invalid approaches include selecting arbitrary parameter values or distribution shapes that do not reflect physical properties. In this problem, we will use data to inform our model. Any time one works with data, one should first perform *exploratory data analysis*---as we will see below, potential issues can be lurking in the data.\n",
    "\n",
    "#### __Q4: Inspect the data__\n",
    "\n",
    "In this exercise, we will behave as though we have data available from our supplier; in reality the data come from Stang et al. [2]. Given the supplier data `df_stang`, these data are for the raw stock and do not represent any material property changes arising from our own manufacturing process. In practice, we should document observations like this; if discrepancies arise between the model and physical testing of our plates, this step in the model development may be important.\n",
    "\n",
    "Before trying to fit a distribution, it is always a good idea to inspect the data to inform a model choice. We will use the built-in dataset from Stang et al. [2] and the Python package `Seaborn` to visualize the data and inform our joint distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Visualize the data\n",
    "# TODO: Use the seaborn function sns.pairplot()\n",
    "#       to visualize the data.\n",
    "# NOTE: Remember that Shift + Tab will access\n",
    "#       the documentation of a function at the\n",
    "#       cursor.\n",
    "###\n",
    "\n",
    "# -- NO NEED TO MODIFY THIS CODE ----\n",
    "from grama.data import df_stang\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.pairplot(data=df_stang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn automatically plots all the numerical variables against each other in a [*scatterplot matrix*](https://en.wikipedia.org/wiki/Scatter_plot#Scatter_plot_matrices). This gives us a quick look at the rough marginal shapes of the data, and a sense of the dependence between variables.\n",
    "\n",
    "Inspecting this pairplot, we can see there is a bulge of *outliers* in (`E, mu`) space. A bit more data exploration reveals a striking pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_stang, hue=\"thick\", vars=[\"E\", \"mu\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "- The supplier's plates at `thick == 0.081` have strange behavior! They tend to be less stiff, which could lead to earlier buckling.\n",
    "- Setting `t` to deterministic in our model essentially assumes independence with `E, mu`. This may not be a valid assumption.\n",
    "- Note however that the discrepancy is small, relative to the typical elasticity; only about `5%` of the typical value.\n",
    "\n",
    "In practice, we should do two things:\n",
    "\n",
    "1. Probe the impact of these data features on our model results. This will tell us whether the feature in the data is *meaningful* for our purposes.\n",
    "1. Revisit the data; try to determine / understand the cause of this pattern. This step is contingent on findings from the previous step.\n",
    "\n",
    "To address point (1), we will fit *two* models---one with and one without the thick plates. We will then compare the results to see if this feature in the data appreciably affects our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a distribution\n",
    "\n",
    "---\n",
    "\n",
    "[Skylar's Theorem](https://en.wikipedia.org/wiki/Copula_(probability_theory)#Sklar's_theorem) justifies the modeling of *any* joint distribution in terms of *marginals* and a *copula*. Thus, we will decompose the fitting of a distribution into two steps: first fitting marginals, and then fitting a copula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting marginals <a id=\"s3.2\"></a>\n",
    "\n",
    "The `py_grama` package provides utilities to fit \"named\" distributions to given data. We will use this simple approach to model the variability observed above. Note that *this approach adds additional information beyond the data*---fitting a distribution *asserts* a shape for the distribution. See the documentation chapter on [Random Variable Modeling](https://py-grama.readthedocs.io/en/latest/source/rv_modeling.html) for more information.\n",
    "\n",
    "Based on the EDA above, we may choose to fit a normal distribution (`norm`) for `E` and a Beta distribution (`beta`) for `mu`. You will do this in the following exercise.\n",
    "\n",
    "#### __Q5: Fit marginal distributions__\n",
    "\n",
    "Building on `md_plate`, fit marginals for `E, mu` based on the filtered and unfiltered Stang et al. data. Compare the fitted parameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Add marginals\n",
    "# TODO: Use gr.comp_marginals() to fit marginal distributions for\n",
    "#       E, mu. Provide two versions of the model; one with the\n",
    "#       Stang et al. data filtered for thick < 0.08, and one\n",
    "#       with all the data. Provide these models as md_filtered\n",
    "#       and md_full, respectively.\n",
    "#\n",
    "#       Use the function gr.marg_named() to compute\n",
    "#       valid marginals for gr.comp_marginals(). Fit a normal (\"norm\")\n",
    "#       for `E`, and a Beta (\"beta\") for `mu`.\n",
    "#\n",
    "# NOTE: You will need to pass univariate data to gr.marg_named();\n",
    "#       You can access a single column \"var\" of a Pandas DataFrame\n",
    "#       with either subscript (df.var) or key (df[\"key\"]) notation.\n",
    "###\n",
    "\n",
    "# -- NO NEED TO MODIFY THIS CODE ----\n",
    "# Filter data for the second model\n",
    "df_filtered = df_stang[df_stang.thick < 0.08]\n",
    "\n",
    "\n",
    "md_full = md_plate >> gr.cp_marginals(\n",
    "    E=gr.marg_named(df_stang.E, \"norm\"), mu=gr.marg_named(df_stang.mu, \"beta\")\n",
    ")\n",
    "\n",
    "md_filtered = md_plate >> gr.cp_marginals(\n",
    "    E=gr.marg_named(df_filtered.E, \"norm\"),\n",
    "    mu=gr.marg_named(df_filtered.mu, \"beta\"),\n",
    ")\n",
    "md_filtered.printpretty()\n",
    "md_full.printpretty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the marginal parameter values are listed by `printpretty()`; this allows us to see that the location of `E` is largely unchanged, but its `scale` is rather dramatically affected. Changes in the `beta` distribution are more difficult to read from this output; ultimately we will make a judgement in terms of the distribution of function outputs.\n",
    "\n",
    "However, before we can analyze this model we must define a *copula* structure; if we attempt to perform a Monte Carlo on our model as-defined:\n",
    "\n",
    "```\n",
    "gr.eval_monte_carlo(md_full, df_det=\"nom\")\n",
    "```\n",
    "\n",
    "we will once again find the error:\n",
    "\n",
    "```\n",
    "ValueError:\n",
    "Present model copula must be defined for sampling.\n",
    "Use CopulaIndependence only when inputs can be guaranteed\n",
    "independent. See the Documentation chapter on Random\n",
    "Variable Modeling for more information.\n",
    "```\n",
    "\n",
    "This is because `py_grama` is *explicit* about dependency structures. We will fit a copula with `py_grama` tools below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a copula <a id=\"s3.3\"></a>\n",
    "\n",
    "A [*copula*](https://en.wikipedia.org/wiki/Copula_(probability_theory)), practically speaking, is a tool to define the dependency structure between random variables. See the [Random Variable Modeling](https://py-grama.readthedocs.io/en/latest/source/rv_modeling.html) documentation chapter for more info. While there are many types of copulas, we will use the simple *Gaussian copula* model below.\n",
    "\n",
    "#### __Q6: Fit a copula__\n",
    "\n",
    "Building on `md_filtered` and `md_full`, fit a Gaussian copula for the dependence structure using `gr.comp_copula_gaussian()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Fit a copula\n",
    "# TODO: Use gr.comp_copula_gaussian() to fit a gaussian copula\n",
    "#       for both md_filtered and md_full.\n",
    "###\n",
    "\n",
    "\n",
    "md_filtered = md_filtered >> gr.cp_copula_gaussian(df_data=df_filtered)\n",
    "md_full = md_full >> gr.cp_copula_gaussian(df_data=df_stang)\n",
    "md_full.printpretty()\n",
    "md_filtered.printpretty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the copula correlation is considerably larger for the full dataset, as compared with the filtered case.\n",
    "\n",
    "With copulas defined, the grama models are now complete! We finish this tutorial with an illustration of model analysis and comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model <a id=\"s4\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "To conclude this exercise, we illustrate how to evaluate and compare the two models. The following example code constructs histograms for the limit state, drawn from both models' distributions and evaluating each model's function. We will use the nominal input values from `md_ref` as the relevant `df_det` settings for our investigation.\n",
    "\n",
    "The following code illustrates how to carry out such an analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DEMONSTRATION: NO NEED TO MODIFY THIS CODE ----\n",
    "# Setup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 1e4\n",
    "df_det = md_ref >> gr.ev_nominal(df_det=\"nom\")\n",
    "\n",
    "# Evaluate models\n",
    "df_filtered = md_filtered >> gr.ev_monte_carlo(\n",
    "    n=n, df_det=df_det[md_ref.var_det]\n",
    ")\n",
    "df_filtered[\"source\"] = \"filtered\"\n",
    "\n",
    "df_full = md_full >> gr.ev_monte_carlo(n=n, df_det=df_det[md_ref.var_det])\n",
    "df_full[\"source\"] = \"full\"\n",
    "\n",
    "## Gather data and plot\n",
    "df_compare = pd.concat((df_filtered, df_full), axis=0)\n",
    "g = sns.FacetGrid(df_compare, row=\"source\", sharex=True, sharey=False)\n",
    "g.map(plt.hist, \"g_buckle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the `filtered` case has a considerably narrower distribution than the `full` case, as we might expect based on the input distribution. We can make a quantitative comparison by summarizing the output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DEMONSTRATION: NO NEED TO MODIFY THIS CODE ----\n",
    "# Quantitative comparison\n",
    "print(\"Filtered\")\n",
    "print(df_filtered[md_filtered.out].describe())\n",
    "print()\n",
    "print(\"Full\")\n",
    "print(df_full[md_full.out].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $g > 0$ corresponds to safe operation. At least at the nominal conditions in which we're interested, both cases are fairly safe. \n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Based on the quantitative results and our stated use-case, the difference between the `filtered` and `full` model cases is negligible. We should be able to use either model for our intended purpose, as we will arrive at similar conclusions.\n",
    "\n",
    "If we were interested instead in *optimizing* the plate geometry, our conclusion would be very different. Carrying out an uncertainty analysis using the two models above could lead to very different designs, in which case we would need to think more carefully about the difference between the two. Further investigation of the data and manufacturing processes would be necessary to support a more detailed analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "When you are done, please take [this survey](https://docs.google.com/forms/d/e/1FAIpQLSd15FQseSKMxYVSY1dLny0AKXeOPxc6RJVDbwecYALouWHsBQ/viewform?entry.923399158=4088579) about the exercise.\n",
    "\n",
    "When you're ready, move on to [Tutorial 04: Hierarchical Functions](https://github.com/zdelrosario/py_grama/blob/master/tutorials/t04_dag_assignment.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "---\n",
    "\n",
    "[1] David J Peery. Aircraft structures. Courier Corporation, 2011.\n",
    "\n",
    "[2] Ambrose H. Stang, Martin Greenspan, and Sanford B. Newman, \"Poisson's Ratio of Some Structural Alloys for Large Strains\" (1946) *Journal of Research of the National Bureau of Standards*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
